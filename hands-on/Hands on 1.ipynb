{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/FFFreitas/2nd-Workshop-on-Compact-Objects-Gravitational-Waves-and-Deep-Learning/blob/main/hands-on/Hands%20on%201.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figs/celebahq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative Adversarial Networks (GANs) have brought about a revolutionary storm in the machine learning (ML) community. They, to some extent, have changed the way people solve practical problems in Computer Vision (CV) and Natural Language Processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start exploiting the new features in PyTorch, we will first learn to build a simple GAN with NumPy to generate sine signals so that you may have a profound understanding of the mechanism beneath GANs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning – classification and generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML is the study of recognizing patterns from data without hardcoded rules given by humans. The recognizing of patterns (**Pattern Recognition** or **PR**) is the automatic discovering of the similarities and differences among raw data, which is an essential way to realize **Artificial Intelligence** (**AI**) that only exists in novels and movies.\n",
    "\n",
    "Although it is hard to tell when exactly real AI will come to birth in the future, the development of ML has given us much confidence in recent years. ML has already been vastly used in many fields, such as CV, NLP, recommendation systems, Intelligent Transportation Systems (ITS), medical diagnoses, robotics, and advertising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ML model is typically described as a system that takes in data and gives certain outputs based on the parameters it contains. The **learning** of the model is actually adjusting the parameters to get better outputs (a.k.a. parametric learning).\n",
    "\n",
    "![](./figs/learning_method.png)\n",
    "\n",
    "\n",
    "We then use one or several criteria to measure the output, to tell how well our model performs. In this step, a set of desired outputs (or ground truth) with respect to the training data would be very helpful. If ground truth data is used in training, this process is often called supervised learning. If not, it is often regarded as unsupervised learning.\n",
    "_____\n",
    "A parametric model is characterized by having a fixed number of parameters, whereas a nonparametric model’s number of parameters is infinite (determined by data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We constantly adjust the model's parameters based on its performance (in other words, whether it gives us the results we want) so that it yields better results in the future. This process is called model training. The training of a model takes as long as it pleases us. Typically, we stop the training after a certain number of iterations (epochs) or when the performance is good enough.\n",
    "\n",
    "What types of problems this model can solve is essentially determined by the types of input and output data we want. For example, a classification model takes an input of any number of dimensions (audio, text, image, or video) and gives a 1-dimension output (single values indicating the predicted labels).\n",
    "\n",
    "A generative model typically takes a 1-dimension input (a latent vector) and generates high-dimension outputs (images, videos, or 3D models). It maps low-dimensional data to high-dimensional data, at the same time, trying to make the output samples look as convincing as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing adversarial learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditionally, generative problems are solved by statistics-based methods such as a Boltzmann machine, Markov chain, or variational encoder. As mathematically profound as they are, the generated samples are as of yet far from perfect.\n",
    "\n",
    "A classification model maps high-dimension data to low-dimension, while a generative model often maps low-dimension data to high-dimension ones.\n",
    "\n",
    "Can we get the two different models to work against each other and improve themselves at the same time?\n",
    "\n",
    "![](./figs/gan.svg)\n",
    "\n",
    "\n",
    "If we take the output of a generative model as the input of the classification model, we can measure the performance of the generative model (the armor) with the classification model (the sword). At the same time, we can improve the classification model (the sword) by feeding generated samples (the armor) along with real samples, since we can agree that more data is often better for the training of ML models.\n",
    "\n",
    "The training process where the two models try to weaken each other and, as a result, improve each other is called adversarial learning.\n",
    "\n",
    "GANs are designed based on this very idea, which was proposed by Goodfellow, Pouget Abadie, Mirza, et al in 2014. Now, GANs have become the most thriving and popular method to synthesize audio, text, images, video, and 3D models in the ML community. In this book, we will walk you through the basic components and mechanisms of different types of GANs and learn how to use them to address various practical problems. In the next section, we will introduce the basic structure of GANs to show you how and why they work so well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator and discriminator networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical structure of a GAN contains two different networks: a **generator** network and a **discriminator** network. The **generator** network typically takes random noises as input and generates fake samples. Our goal is to let the fake samples be as close to the real samples as possible. That's where the **discriminator** comes in. The **discriminator** is, in fact, a classification network, whose job is to tell whether a given sample is fake or real.\n",
    "\n",
    "The generator tries its best to trick and confuse the discriminator to make the wrong decision, while the discriminator tries its best to distinguish the fake samples from the real ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this process, the differences between fake and real samples are used to improve the generator. Therefore, the generator gets better at generating realistic-looking samples while the discriminator gets better at picking them out. Since real samples are used to train the discriminator, the training process is therefore supervised. Even though the generator always gives fake samples without the knowledge of ground truth, the overall training of GAN is still **supervised**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, the discriminator outputs a scalar prediction $o \\in \\mathbb{R}$  for a given input, such as using a dense layer with hidden size 1, and then applies sigmoid function to obtain the predicted probability $D(\\mathbf x) = 1/(1+e^{-o})$. Assume the label $y$ for the true data is 1 and 0 for the fake data. We train the discriminator to minimize the cross-entropy loss, i.e.,\n",
    "\n",
    "$$\\min_D \\{ - y \\log D(\\mathbf x) - (1-y)\\log(1-D(\\mathbf x)) \\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the generator, it first draws some parameter $\\bf{z}\\in \\mathbb{R}^d$ from a source of randomness, e.g., a normal distribution $\\bf{z} \\sim \\mathbb{N}(0,1)$. We often call $\\bf{z}$ as the latent variable. It then applies a function to generate $x` = G(\\bf{z})$. The goal of the generator is to fool the discriminator to classify $x` = G(\\bf{z})$ as true data, i.e., we want $D(G(\\bf{z})) \\approx 1$. In other words, for a given discriminator $D$, we update the parameters of the generator $G$ to maximize the cross-entropy loss when $y=0$, i.e.,\n",
    "\n",
    "\n",
    "$$\\max_G \\{ - (1-y) \\log(1-D(G(\\mathbf z))) \\} = \\max_G \\{ - \\log(1-D(G(\\mathbf z))) \\}$$\n",
    "\n",
    "\n",
    "If the generator does a perfect job, then $D(\\mathbf x')\\approx 1$ so the above loss near 0, which results the gradients are too small to make a good progress for the discriminator. So commonly we minimize the following loss:\n",
    "\n",
    "$$\\min_G \\{ - y \\log(D(G(\\mathbf z))) \\} = \\min_G \\{ - \\log(D(G(\\mathbf z))) \\}$$\n",
    "\n",
    "which is just feed $\\mathbf x'=G(\\mathbf z)$ into the discriminator but giving label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, $D$ and $G$ are playing a “minimax” game with the comprehensive objective function:\n",
    "\n",
    "$$min_D max_G \\{ -E_{x \\sim \\text{Data}} log D(\\mathbf x) - E_{z \\sim \\text{Noise}} log(1 - D(G(\\mathbf z))) \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import everything we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Some “Real” Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our generator network will be the simplest network possible - a single layer linear model. This is since we will be driving that linear network with a Gaussian data generator. Hence, it literally only needs to learn the parameters to fake things perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the discriminator we will be a bit more discriminating: we will use an MLP with 3 layers to make things a bit more interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We need to build the training loop for our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The generator is updated similarly. Here we reuse the cross-entropy loss but change the label of the fake data from  0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Both the discriminator and the generator performs a binary logistic regression with the cross-entropy loss. We use Adam to smooth the training process. In each iteration, we first update the discriminator and then the generator. We visualize both losses and generated examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fastai:V2",
   "language": "python",
   "name": "fastai_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
