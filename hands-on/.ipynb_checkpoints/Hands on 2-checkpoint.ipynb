{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/FFFreitas/2nd-Workshop-on-Compact-Objects-Gravitational-Waves-and-Deep-Learning/blob/main/hands-on/Hands%20on%202.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Your First GAN with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Convolutional GANs\n",
    "\n",
    "DCAGN (Deep Convolutional Generative Adversarial Network) is one of the early well-performing and stable approaches to generate images with adversarial training. Even when we only train a GAN to manipulate 1D data, we have to use multiple techniques to ensure a stable training. A lot of things could go wrong in the training of GANs. For example, either a generator or a discriminator could overfit if one or the other does not converge. Sometimes, the generator only generates a handful of sample varieties. This is called mode collapse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure the stable training of GANs on image data like this, a DCGAN uses three techniques:\n",
    "\n",
    "- Getting rid of fully connected layers and only using convolution layers \n",
    "- Using strided convolution layers to perform downsampling, instead of using pooling layers\n",
    "- Using ReLU/leakyReLU activation functions instead of Tanh between hidden layers\n",
    "\n",
    "In this lecture, we will introduce the architectures of the generator and discriminator of the DCGAN and learn how to generate images with it. We'll use a spectrogram dataset I generated for this lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The architecture of generator\n",
    "\n",
    "The generator network of a DCGAN contains 4 hidden layers (we treat the input layer as the 1 st hidden layer for simplicity) and 1 output layer. Transposed convolution layers are used in hidden layers, which are followed by batch normalization layers and ReLU activation functions. The output layer is also a transposed convolution layer and Tanh is used as the activation function. The architecture of the generator is shown in the following diagram:\n",
    "\n",
    "![](../figs/DCGAN_gen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $2^{nd} , 3^{rd}$ , and $4^{th}$ hidden layers and the output layer have a stride value of 2. The 1 st layer has a padding value of 0 and the other layers have a padding value of 1. As the image (feature map) sizes increase by two in deeper layers, the numbers of channels are decreasing by half. This is a common convention in the architecture design of neural networks. All kernel sizes of transposed convolution layers are set to 4 x 4. The output channel can be either 1 or 3, depending on whether you want to generate grayscale images or color images.\n",
    "\n",
    "## !!!!\n",
    "The transposed convolution layer can be considered as the reverse process of a normal convolution. It was once called by some a deconvolution layer, which is misleading because the transposed convolution is not the inverse of convolution. Most convolution layers are not invertible, because they are ill-conditioned (have extremely large condition numbers) from the linear algebra perspective, which makes their pseudoinverse matrices unfit for representing the inverse process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The architecture of a discriminator\n",
    "\n",
    "The discriminator network of a DCGAN consists of 4 hidden layers (again, we treat the input layer as the 1 st hidden layer) and 1 output layer. Convolution layers are used in all layers, which are followed by batch normalization layers except that the first layer does not have batch normalization. LeakyReLU activation functions are used in the hidden layers and Sigmoid is used for the output layer. The architecture of the discriminator is shown in the following:\n",
    "\n",
    "![](../figs/DCGAN_disc.png)\n",
    "\n",
    "The input channel can be either 1 or 3, depending on whether you are dealing with grayscale images or color images. All hidden layers have a stride value of 2 and a padding value of 1 so that their output image sizes will be half the input images. As image sizes increase in deeper layers, the numbers of channels are increasing by twice. All kernels in convolution layers are of a size of 4 x 4. The output layer has a stride value of 1 and a padding value of 0. It maps 4 x 4 feature maps to single values so that the Sigmoid function can transform the value into prediction confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a DCGAN with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = True\n",
    "DATA_PATH = 'datasets/pure/'\n",
    "OUT_PATH = 'ouputs_2/'\n",
    "log_file = os.path.join(OUT_PATH, 'log.txt')\n",
    "BATCH_SIZE = 128\n",
    "IMAGE_CHANNEL = 3\n",
    "Z_DIM = 100\n",
    "G_HIDDEN = 64\n",
    "X_DIM = 64\n",
    "D_HIDDEN = 64\n",
    "EPOCH_NUM = 100\n",
    "REAL_LABEL = 1\n",
    "FAKE_LABEL = 0\n",
    "lr = 2e-4\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have a CUDA-enabled graphics card and want to train the networks on the CPU, you can change CUDA to False . DATA_PATH points to the root directory of our image dataset. BATCH_SIZE has a major impact on how much GPU memory your code will consume. If you are not sure what batch size is appropriate for your system, you can start at a small value, train your model for 1 epoch, and double the batch size until errors pop up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Logging to {log_file}\")\n",
    "CUDA = CUDA and torch.cuda.is_available()\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if CUDA:\n",
    "    print(f\"CUDA version: {torch.version.cuda}\\n\")\n",
    "if seed is None:\n",
    "    seed = np.random.randint(1,10000)\n",
    "print(f\"random seed: {seed}\")\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "cudnn.benchmark = True\n",
    "device = torch.device(\"cuda:0\" if CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator network\n",
    "Now, let's define the generator network with PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output layer does not have a batch normalization layer connected to it.\n",
    "\n",
    "Let's create a helper function to initialize the network parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0., 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1., 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator network\n",
    "\n",
    "Now, let's define the discriminator network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the input layer does not have a batch normalization layer connected to it. This is because, when applying batch normalization to all layers, it could lead to sample oscillation and model instability, as pointed out in the original paper.\n",
    "\n",
    "Similarly, we can create a Discriminator object as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and evaluation\n",
    "\n",
    "We will use Adam as the training method for both the generator and discriminator networks. Let's first define the loss function for the discriminator network and optimizers for both of the networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset consist of spectrograms from different waveforms generated from pycbc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ex = Image.open('datasets/pure/0_17.4_29.0_1092_2.4749_2.9193_-0.7559_3.9973.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ex.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let's create the dataset using the *Dataset* module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGravGan(Dataset):\n",
    "    def __init__(self, path_to_data, transform=None):\n",
    "        self.path_to_data = Path(path_to_data)\n",
    "        self.transform = transform\n",
    "        self.list_of_images = list(self.path_to_data.glob('*.png'))\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        img = Image.open(self.list_of_images[i])\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = transforms.ToTensor()(img)\n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.list_of_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GW_dataset = DataGravGan('datasets/pure/', transform=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GW_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GW_dataset[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(GW_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training iteration\n",
    "\n",
    "1. Train the discriminator with the real data and recognize it as real.\n",
    "2. Train the discriminator with the fake data and recognize it as fake.\n",
    "3. Train the generator with the fake data and recognize it as real.\n",
    "\n",
    "The first two steps let the discriminator learn how to tell the difference between real data and fake data. The third step teaches the generator how to confuse the discriminator with generated samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fastai:V2",
   "language": "python",
   "name": "fastai_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
